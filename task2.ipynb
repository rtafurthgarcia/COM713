{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b13f815",
   "metadata": {},
   "source": [
    "# Task 1 Perspectives on Python-oriented SBOM Generation Tools\n",
    "Task 1 analysed current SBOM generation practices in the Python ecosystem and identified a fundamental limitation: existing tools rely primarily on metadata files or semi-dynamic installation processes, leading to incomplete dependency discovery and inconsistent results. Through critical synthesis of recent academic work, the first part motivates a new SBOM generation approach that shifts the focus from metadata-centric analysis to source-code–level dependency discovery.\n",
    "This part is now dedicated to the implementation of our novel approach. Using Python’s Abstract Syntax Tree (AST) to extract imports directly from source code and construct a dependency graph, we complement existing methods and seek to achieve 100% completeness. \n",
    "The first section is about the project's chosen datasets, why and how they've been merged. The second section is about the choice of data structure appropriate for graph problems. Goodrich et al.[2] provide us with four possible data structures\n",
    "\n",
    "## Nota bene\n",
    "1. In the first task, under Comparison and metrics, we suggest correctness will be also assessed. This was a mistake; the focus of this project remains exclusively about completeness, as in capturing all dependencies.  \n",
    "2. This project revolving around packages, ***unless `requirements.txt` is ran in a virtualenv with python 3.11 at least, \n",
    "the AST analysis part won't work***, for it is the source code of the different required packages that is analysed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e98462",
   "metadata": {},
   "source": [
    "## 1 Data structures \n",
    "One could conceive the data structure defining how packages relate to one another naively as a tree, where each package may have children-packages. In fact, it is often referred in the literature as a dependency tree. However, \n",
    "this term misrepresents the true nature of software dependencies, for packages rarely have a single parent relationship or even form (problematic) cycles, per Tellnes' own introduction of the problem[1]. Thus, in professional software development, dependency graphs are the norm. For this reason, the core data structure featured in this document will be shaped by graph theory. Goodrich et al.[2] provide us with four possible data structures: \n",
    "1. edge lists \n",
    "2. adjacency list\n",
    "3. adjacency map\n",
    "4. adjacency matrix \n",
    "\n",
    "Whilst in the task n°1 it had been concluded that the adjacency list had to be used for NodeVisitor depended on it, \n",
    "it turned out that `ast.NodeVisitor` doesnt really expect a specific type. We were thus free to choose any of those data structures. One could choose the edge list on a whim, for it is easy to implement, and has the most optimal for the three functions we would use (`vertices()` being O(n), `insert_vertices()` and `insert_edges()` being O(1)), as described in [2, p.627], to construct the graph and compare it against others. But doing so would be a mistake; we cannot forget some projects might have cycles and for this reason we need to be able to avoid duplicates. Replacing the lists by sets would mean both the getter and setter necessary to check for duplicates before adding are O(n) too[3]. Therefore it would have best to use a adjacency map, for its getter `get_edge()` is O(1), but our dataset needs to exported and merged too, implying serialisation is a major challenge for behavior-heavy objects like graphs. Bad experience has been made about this last part, as visible in our chatgpt transcript[4]. Hence we reverted back to the first option that was the edge lists, but with sets instead, for we will only implement a small subset of functions where O(1) applies. Sets allow for easy comparisons, perfect for later analyses. It is important to note that python's sets make us of hashmaps, which for insertion and look-ups are worth O(1) too. We follow Goodrich et al.'s edge list implementation otherwise[3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7f3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "@dataclass(eq=True, frozen=True)\n",
    "class Package():\n",
    "    name: str\n",
    "    #node: Any | None = None # only assigned something else when parsing with the AST algorithm \n",
    "\n",
    "    @staticmethod\n",
    "    # O(c) => \"simple\" deserialisation\n",
    "    def from_dict(d: dict) -> \"Package\":\n",
    "        return Package(**d)\n",
    "\n",
    "@dataclass(eq=True, frozen=True)\n",
    "class ImportStatement():\n",
    "    who_imports: Package\n",
    "    who_is_imported: Package\n",
    "\n",
    "    @staticmethod\n",
    "    # O(c) => constant time\n",
    "    def from_dict(d: dict) -> \"ImportStatement\":\n",
    "        return ImportStatement(\n",
    "            who_imports=Package.from_dict(d[\"imports\"]),\n",
    "            who_is_imported=Package.from_dict(d[\"imported\"]),\n",
    "        )\n",
    "    \n",
    "@dataclass(eq=True, frozen=True)\n",
    "class DependencyGraph():\n",
    "    packages: set[Package] = field(default_factory=set)\n",
    "    import_statements: set[ImportStatement] = field(default_factory=set)\n",
    "\n",
    "    # O(c) => simple append, constant time\n",
    "    def insert_package(self, package_name: str, node: Any | None = None) -> Package:\n",
    "        new_package = Package(package_name)\n",
    "        self.packages.add(new_package)\n",
    "\n",
    "        return new_package\n",
    "\n",
    "    # O(c) => simple append, constant time\n",
    "    def insert_importstatement(self, imports: Package, imported: Package):\n",
    "        new_importstatement = ImportStatement(imports, imported)\n",
    "        self.import_statements.add(new_importstatement)\n",
    "\n",
    "        return new_importstatement\n",
    "    \n",
    "    # O(n) => linear time because we have to look through the whole list\n",
    "    def imports(self, package: Package) -> list[ImportStatement]:\n",
    "        return [statement for statement in self.import_statements if statement.who_imports == package]\n",
    "    \n",
    "    @staticmethod\n",
    "    # O(2n) => linear time still but we have to loop through two(2) lists\n",
    "    def from_dict(d: dict) -> \"DependencyGraph\":\n",
    "        return DependencyGraph(\n",
    "            packages=set(Package.from_dict(p) for p in d[\"packages\"]),\n",
    "            import_statements=set(\n",
    "                ImportStatement.from_dict(i)\n",
    "                for i in d[\"import_statements\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class PackageAnalysis:\n",
    "    source_path: str\n",
    "    graphs: dict[str, DependencyGraph]\n",
    "    raw_packages_from_metadata: list[str]\n",
    "    ground_truth: DependencyGraph | None\n",
    "\n",
    "    @staticmethod\n",
    "    # O(n^2) => quadratic time, and looping\n",
    "    # 1 for instantiation\n",
    "    # 1 for return\n",
    "    def from_dict(d: dict) -> \"PackageAnalysis\":\n",
    "        return PackageAnalysis(\n",
    "            source_path=d[\"source_path\"],\n",
    "            graphs={\n",
    "                k: DependencyGraph.from_dict(v)\n",
    "                for k, v in d[\"graphs\"].items()\n",
    "            },\n",
    "            raw_packages_from_metadata=d[\"raw_packages_from_metadata\"],\n",
    "            ground_truth=(\n",
    "                DependencyGraph.from_dict(d[\"ground_truth\"])\n",
    "                if d[\"ground_truth\"] is not None\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    package_analyses: dict[str, PackageAnalysis] = field(default_factory=dict)\n",
    "\n",
    "    @staticmethod\n",
    "    # O(n^3) => cubic time because we have to loop through multiple nested objects\n",
    "    def from_dict(d: dict) -> \"Dataset\":\n",
    "        return Dataset(\n",
    "            package_analyses={\n",
    "                k: PackageAnalysis.from_dict(v)\n",
    "                for k, v in d[\"package_analyses\"].items()\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a3790",
   "metadata": {},
   "source": [
    "## 2.0 Datasets and setup \n",
    "In the subsequent task, we examined and selected two datasets that allow us to draw a comparison between our tools and our new approach. Both datasets contain the source code of different pacakges to analyse. Due to the sheer size of those datasets (multiple GBs), they are not included in this file directly and can be consulted on the (project's github repository)[https://github.com/rtafurthgarcia/COM713].\n",
    "Our datasets (`\\ds1` and `\\ds2`) share the same structure:\n",
    "- `\\packages` contains the packages to analyse and to generate SBOMs from, \n",
    "- `\\sbom` contains the generated SBOMs generated by each tool for each package, and serve as comparison source.\n",
    "\n",
    "### 2.1 Dataset n°1\n",
    "Dataset n1 (ds1) is a copy from Cofano et al. Dependencies are read from `requirements.txt` from `\\sbom`. This dataset contains no ground truth, and only `\\sbom` can be used to draw a comparison between our new approach and the other tools. \n",
    "Here, `COM713`, a package specially crafted to avoid detection by regular metadata-only tools has been added in the `requirements.txt` files. This will allow us to test that our AST analyser is immune to parser confusion.\n",
    "\n",
    "### 2.2 Dataset n°2\n",
    "Dataset n2 (ds2) is a copy from Jia et al's dataset. `\\deptree_gt` contains the ground truth as json files for each package to compare with the other tools real performance in `\\sbom`\n",
    "\n",
    "### 2.3 Preprocessing and merging\n",
    "These two datasets have been parsed and merged externally; the process required a cyclonedx library that couldnt be attached to this project. However, if curious as to how it worked, you can peek into the `merge.py` file and see how it got done. It required serialising our dataclasses from above into json files. By merging is meant combining all previous relevant dataset files into one, but we keep both datasets distinct for practical reasons. Deserialising those datasets meant implementing `from_dict` functions as to retroactively convert all nested objects into their original types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c46c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset1: Dataset\n",
    "dataset2: Dataset\n",
    "\n",
    "with open(\"merged_ds1.json\", \"r\") as dataset_file:\n",
    "    dataset1 =  Dataset.from_dict(json.loads(dataset_file.read())) \n",
    "\n",
    "with open(\"merged_ds2.json\", \"r\") as dataset_file:\n",
    "    dataset2 =  Dataset.from_dict(json.loads(dataset_file.read())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345897cd",
   "metadata": {},
   "source": [
    "## 3. AST Algorithm\n",
    "Our AST algorithm follows this logic: \n",
    "\n",
    "```\n",
    "                                                                               ┌───────────────────┐     \n",
    "                                                                               │                   │     \n",
    "                                                                ┌────────┐     │                   │     \n",
    "                                                                │        │     │ Print the SBOM    │     \n",
    "                                                                │ Start ◄┼─────┼ and export it as a│     \n",
    "                                                                │        │     │ file             ◄┼┐    \n",
    "                                                                └────────┘     │                   ││    \n",
    "                                                                               └───────────────────┘│    \n",
    "                                                                                                    │    \n",
    "                                                                                                   No    \n",
    "                                                                                                    │    \n",
    "                                                                                                    │    \n",
    "                                                                                                 xxx│x   \n",
    "                                                                             xxxxx              xx   xx  \n",
    "                  ┌────────────────┐        ┌────────────────────┐          xx   xx            xx     xx \n",
    "                  │                │        │                    │         xx     xx          xx Any   xx\n",
    "┌────────┐        │                │        │ Look through the   │        xx Any   xx         x  .py    x\n",
    "│        │        │  Read source   │        │ AST for import     │        x  import ───No─────►x left? xx\n",
    "│ Start ─┼────────┼► file (.py),   ┼────────► statements         ┼────────►x left? xx          xx     xx \n",
    "│        │        │  build the AST │        │                    │         xx     xx            xx   xx  \n",
    "└────────┘        │                │        │                    │          xx   xx              xx xx   \n",
    "                  └──────▲─────────┘        └────────▲───────────┘           xx xx                │xx    \n",
    "                         │                           │                        x│x                 │      \n",
    "                         │                           │                         │                  │      \n",
    "                         │                           │                         │                  │      \n",
    "                         │                           │                        Yes                Yes     \n",
    "                         │                  ┌────────┼───────────┐             │                  │      \n",
    "                         │                  │                    │             │                  │      \n",
    "                         │                  │  Add to graph      │             │                  │      \n",
    "                         │                  │                   ◄│─────────────┘                  │      \n",
    "                         │                  │                    │                                │      \n",
    "                         │                  │                    │      ┌────────────────────┐    │      \n",
    "                         │                  └────────────────────┘      │                    │    │      \n",
    "                         │                                              │  Move to next      │    │      \n",
    "                         └──────────────────────────────────────────────│  source file,      │────┘      \n",
    "                                                                        │  mark file as read │           \n",
    "                                                                        │                    │           \n",
    "                                                                        └────────────────────┘           \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bb7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\n",
      "main\n",
      "main\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "from importlib.util import find_spec\n",
    "from importlib.metadata import packages_distributions\n",
    "\n",
    "class PackageAnalyser(ast.NodeVisitor):\n",
    "    \"\"\"\n",
    "    Parses the package's sourcecode, build an abstract syntax tree, and from this, identifies the imports \n",
    "    that will allow to dive deeper into each imported package. This process is what generates our dependency graph, \n",
    "    where each Vertex or Package will be an entry in our SBOM.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_path: str):\n",
    "        self.source_path = source_path\n",
    "        self.graph = DependencyGraph()\n",
    "        self.current_file = \"\"\n",
    "        self.distribution_packages = packages_distributions()\n",
    "        self._visited_nodes = {}\n",
    "\n",
    "        if os.path.exists(self.source_path):\n",
    "            if (os.path.isfile(self.source_path)) and source_path.endswith(\".py\"):\n",
    "                #self.graph.insert_package(root_package_name)\n",
    "                self.current_file = self.source_path\n",
    "            elif (os.path.isdir(self.source_path)):\n",
    "                self.current_file = [file for file in os.listdir(self.source_path) if file.endswith(\".py\")][0]\n",
    "            else:\n",
    "                raise Exception(\"No source file found\")\n",
    "\n",
    "    \"\"\"\n",
    "    Each visit_x function is called each time a node of a node of our abstract syntax tree is visited\n",
    "    _Module -> is called each time a new module is visited\n",
    "    _Import -> is called each time a new import is visited\n",
    "    _ImportFrom -> is called each time a a new from ... import is visited \n",
    "    \"\"\"\n",
    "    def visit_Module(self, node: ast.Module) -> None:\n",
    "        self._visited_nodes[self.current_file] = node\n",
    "        \n",
    "        package_name = Path(self.current_file).stem\n",
    "\n",
    "        # ultimately, we only want to add to our graph the distribution packages\n",
    "        if (not package_name.startswith(\"__\")):\n",
    "            self.current_package = self.graph.insert_package(package_name)\n",
    "        \n",
    "        self.generic_visit(node)\n",
    "\n",
    "    # O(n!) Really bad, _parse_and_visit will both read, build the AST tree, and look for \n",
    "    # further files to parse by calling itself during the \"visiting\" of the tree\n",
    "    def visit_Import(self, node: ast.Import) -> None:\n",
    "        package_name = node.names[0].name # the first name works\n",
    "\n",
    "        if (not self._is_separate_package(package_name)):\n",
    "            return\n",
    "        \n",
    "        package_path = self._find_package_path(package_name)\n",
    "        self._parse_and_visit(package_path)\n",
    "        self._visited_nodes[package_path] = node\n",
    "        \n",
    "        # ultimately, we only want to add to our graph the distribution packages\n",
    "        if (package_name in self.distribution_packages):\n",
    "            new_package = self.graph.insert_package(package_name)\n",
    "            self.graph.insert_importstatement(\n",
    "                self.current_package, \n",
    "                new_package)\n",
    "            self.current_package = new_package\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    # O(n!) Really bad, _parse_and_visit will both read, build the AST tree, and look for \n",
    "    # further files to parse by calling itself during the \"visiting\" of the tree\n",
    "    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:\n",
    "        package_name = node.module or \"\" # we want the x from 'from x import y', not the y\n",
    "\n",
    "        if (not self._is_separate_package(package_name)):\n",
    "            return\n",
    "        \n",
    "        package_path = self._find_package_path(package_name)\n",
    "        self._parse_and_visit(package_path) # This makes it a recursive process!\n",
    "        self._visited_nodes[package_path] = node\n",
    "\n",
    "        # ultimately, we only want to add to our graph the distribution packages\n",
    "        if (package_name in self.distribution_packages):\n",
    "            new_package = self.graph.insert_package(package_name)\n",
    "            self.graph.insert_importstatement(\n",
    "                self.current_package, \n",
    "                new_package)\n",
    "            self.current_package = new_package\n",
    "\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    # O(1) nothing too special\n",
    "    def _is_separate_package(self, node_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Distinguish between just regular modules, proper modules from separate installed packages\n",
    "        and stdlib packages\n",
    "        \"\"\"\n",
    "        if node_name == '':\n",
    "            return False\n",
    "\n",
    "        if node_name in sys.stdlib_module_names:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            spec = find_spec(node_name)\n",
    "            if spec is None or spec.submodule_search_locations is None:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _find_package_path(self, node_name: str) -> str:\n",
    "        spec = find_spec(node_name)       \n",
    "        if (spec is None or spec.submodule_search_locations is None or spec.origin is None):\n",
    "            raise FileNotFoundError(\"Couldn't find the package's og source\")\n",
    "        else:\n",
    "            return spec.origin\n",
    "        \n",
    "    def _parse_and_visit(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Read the source, parse it to further build the AST, then explore its edges to discover other packages\n",
    "        \"\"\"\n",
    "        if not file_path.endswith(\".py\"):\n",
    "            return \n",
    "        \n",
    "        if file_path in self._visited_nodes: # a look up is constant thx to the fact that its a hashmap!\n",
    "            return \n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\") as source_file:\n",
    "                code = source_file.read()\n",
    "                tree = ast.parse(code)\n",
    "                self.current_file = file_path\n",
    "                self.visit(tree)\n",
    "        except:\n",
    "            return\n",
    "            \n",
    "    \n",
    "    def analyse(self) -> None:\n",
    "        \"\"\"\n",
    "        Will parse the package's source code and build an AST from it, where every node from the tree will be visited\n",
    "        \"\"\"\n",
    "        if (os.path.isfile(self.source_path)):\n",
    "            self._parse_and_visit(self.source_path)\n",
    "        elif (os.path.isdir(self.source_path)):\n",
    "            for root, _, files in os.walk(self.source_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    self._parse_and_visit(file_path)\n",
    "    \n",
    "    def print_packages(self) -> None:\n",
    "        for package in self.graph.packages:\n",
    "            print(package.name)\n",
    "\n",
    "for package, analysis in dataset1.package_analyses.items():\n",
    "    analyser = PackageAnalyser(source_path=analysis.source_path)\n",
    "    analyser.analyse()\n",
    "    analyser.print_packages()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9bf21",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] J. Tellnes, « Dependencies: No Software is an Island », Master thesis, The University of Bergen, 2013. Available on: https://bora.uib.no/bora-xmlui/handle/1956/7540\n",
    "[2] M. T. Goodrich, R. Tamassia, et M. H. Goldwasser, Data structures and algorithms in Python, 1st edition. Hoboken, N.J: Wiley, 2013.\n",
    "[3] « TimeComplexity - Python Wiki ». Consulted the: 4 janvier 2026. [Online]. Available on: https://wiki.python.org/moin/TimeComplexity\n",
    "[4] R. E. L. Tafurth Garcia, « ChatGPT - COM713 », Transcript. [Online]. Available: https://chatgpt.com/share/695ad5cb-35f8-8008-a3a0-d8b0302b4eb2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
