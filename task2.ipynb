{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b13f815",
   "metadata": {},
   "source": [
    "# Task 2: Follow-up from Perspectives on Python-oriented SBOM Generation Tools\n",
    "Task 1 analysed current SBOM generation practices in the Python ecosystem and identified a fundamental limitation: existing tools rely primarily on metadata files or semi-dynamic installation processes, leading to incomplete dependency discovery and inconsistent results. Through critical synthesis of recent academic work, the first part motivates a new SBOM generation approach that shifts the focus from metadata-centric analysis to source-code–level dependency discovery.\n",
    "This part is now dedicated to the implementation of our novel approach. Using Python’s Abstract Syntax Tree (AST) to extract imports directly from source code and construct a dependency graph, we complement existing methods and seek to achieve 100% completeness. \n",
    "The first section is about the choice of data structure appropriate for graph problems. The second section is about the project's chosen datasets, why and how they've been merged. The third one is about the AST analysis itself, focused on the 1st dataset, whilst the second part is mostly about the results' analysis of the 2nd dataset. The final section discusses limitations as well as everything related to asymptotic analysis.\n",
    "\n",
    "## Nota bene\n",
    "1. In the first task, under Comparison and metrics, we suggest correctness will be also assessed. This was a mistake; the focus of this project remains exclusively about completeness, as in capturing all dependencies.  \n",
    "2. This project revolving around packages, to run the AST analyser and obtain the expected results, one needs to copy the ***whole*** repository[https://github.com/rtafurthgarcia/COM713], because the datasets also contain the dependencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e98462",
   "metadata": {},
   "source": [
    "## 1 Data structures \n",
    "One could conceive the data structure defining how packages relate to one another naively as a tree, where each package may have children-packages. In fact, it is often referred in the literature as a dependency tree. However, \n",
    "this term misrepresents the true nature of software dependencies, for packages rarely have a single parent relationship or even form (problematic) cycles, per Tellnes' own introduction of the problem[1]. Thus, in professional software development, dependency graphs are the norm. For this reason, the core data structure featured in this document will be shaped by graph theory. Goodrich et al.[2] provide us with four possible data structures: \n",
    "1. edge lists \n",
    "2. adjacency list\n",
    "3. adjacency map\n",
    "4. adjacency matrix \n",
    "\n",
    "Whilst in the task n°1 it had been concluded that the adjacency list had to be used for NodeVisitor depended on it, \n",
    "it turned out that `ast.NodeVisitor` doesnt really expect a specific type. We were thus free to choose any of those data structures. One could choose the edge list on a whim, for it is easy to implement, and has the most optimal for the three functions we would use (`vertices()` being O(n), `insert_vertices()` and `insert_edges()` being O(1)), as described in [2, p.627], to construct the graph and compare it against others. But doing so would be a mistake; we cannot forget some projects might have cycles and for this reason we need to be able to avoid duplicates. Replacing the lists by sets would mean both the getter and setter necessary to check for duplicates before adding are O(n) too[3]. Therefore it would have best to use a adjacency map, for its getter `get_edge()` is O(1), but our dataset needs to exported and merged too, implying serialisation is a major challenge for behavior-heavy objects like graphs. Bad experience has been made about this last part, as visible in our chatgpt transcript[4]. Hence we reverted back to the first option that was the edge lists, but with sets instead, for we will only implement a small subset of functions where O(1) applies. Sets allow for easy comparisons, perfect for later analyses. It is important to note that python's sets make us of hashmaps, which for insertion and look-ups are worth O(1) too. We follow Goodrich et al.'s edge list implementation otherwise[3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b7f3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "\n",
    "@dataclass(eq=False, frozen=True, unsafe_hash=True)\n",
    "class Package():\n",
    "    name: str\n",
    "\n",
    "    @staticmethod\n",
    "    # O(c) => \"simple\" deserialisation\n",
    "    def from_dict(d: dict) -> \"Package\":\n",
    "        return Package(**d)\n",
    "\n",
    "    def __eq__(self, value) -> bool:\n",
    "        return self.name == value.name\n",
    "\n",
    "@dataclass(eq=True, frozen=True)\n",
    "class ImportStatement():\n",
    "    who_imports: Package\n",
    "    who_is_imported: Package\n",
    "\n",
    "    @staticmethod\n",
    "    # O(c) => constant time\n",
    "    def from_dict(d: dict) -> \"ImportStatement\":\n",
    "        return ImportStatement(\n",
    "            who_imports=Package.from_dict(d[\"imports\"]),\n",
    "            who_is_imported=Package.from_dict(d[\"imported\"]),\n",
    "        )\n",
    "    \n",
    "@dataclass(eq=True, frozen=True)\n",
    "class DependencyGraph():\n",
    "    packages: set[Package] = field(default_factory=set)\n",
    "    import_statements: set[ImportStatement] = field(default_factory=set)\n",
    "\n",
    "    # O(c) => simple append, constant time\n",
    "    def insert_package(self, package_name: str, level: int) -> Package:\n",
    "        new_package = Package(package_name)\n",
    "        self.packages.add(new_package)\n",
    "\n",
    "        return new_package\n",
    "\n",
    "    # O(c) => simple append, constant time\n",
    "    def insert_importstatement(self, imports: Package, imported: Package):\n",
    "        new_importstatement = ImportStatement(imports, imported)\n",
    "        self.import_statements.add(new_importstatement)\n",
    "\n",
    "        return new_importstatement\n",
    "    \n",
    "    # O(n) => linear time because we have to look through the whole list\n",
    "    def imports(self, package: Package) -> list[ImportStatement]:\n",
    "        return [statement for statement in self.import_statements if statement.who_imports == package]\n",
    "    \n",
    "    @staticmethod\n",
    "    # O(2n) => linear time still but we have to loop through two(2) lists\n",
    "    def from_dict(d: dict) -> \"DependencyGraph\":\n",
    "        return DependencyGraph(\n",
    "            packages=set(Package.from_dict(p) for p in d[\"packages\"]),\n",
    "            import_statements=set(\n",
    "                ImportStatement.from_dict(i)\n",
    "                for i in d[\"import_statements\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class PackageAnalysis:\n",
    "    source_path: str\n",
    "    graphs: dict[str, DependencyGraph]\n",
    "    raw_packages_from_metadata: list[str]\n",
    "    packages_path: str\n",
    "    ground_truth: DependencyGraph | None\n",
    "\n",
    "    @staticmethod\n",
    "    # O(n^2) => quadratic time, and looping\n",
    "    # 1 for instantiation\n",
    "    # 1 for return\n",
    "    def from_dict(d: dict) -> \"PackageAnalysis\":\n",
    "        return PackageAnalysis(\n",
    "            source_path=d[\"source_path\"],\n",
    "            graphs={\n",
    "                k: DependencyGraph.from_dict(v)\n",
    "                for k, v in d[\"graphs\"].items()\n",
    "            },\n",
    "            raw_packages_from_metadata=d[\"raw_packages_from_metadata\"],\n",
    "            packages_path=d[\"packages_path\"],\n",
    "            ground_truth=(\n",
    "                DependencyGraph.from_dict(d[\"ground_truth\"])\n",
    "                if d[\"ground_truth\"] is not None\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    package_analyses: dict[str, PackageAnalysis] = field(default_factory=dict)\n",
    "\n",
    "    @staticmethod\n",
    "    # O(n^3) => cubic time because we have to loop through multiple nested objects\n",
    "    def from_dict(d: dict) -> \"Dataset\":\n",
    "        return Dataset(\n",
    "            package_analyses={\n",
    "                k: PackageAnalysis.from_dict(v)\n",
    "                for k, v in d[\"package_analyses\"].items()\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a3790",
   "metadata": {},
   "source": [
    "## 2.0 Datasets and setup \n",
    "In the subsequent task, we examined and selected two datasets that allow us to draw a comparison between our tools and our new approach. Both datasets contain the source code of different pacakges to analyse. Due to the sheer size of those datasets (multiple GBs), they are not included in this file directly and can be consulted on the (project's github repository)[https://github.com/rtafurthgarcia/COM713].\n",
    "Our datasets (`\\ds1` and `\\ds2`) share the same structure:\n",
    "- `\\packages` contains the packages to analyse and to generate SBOMs from, \n",
    "- `\\sbom` contains the generated SBOMs generated by each tool for each package, and serve as comparison source.\n",
    "\n",
    "### 2.1 Dataset n°1\n",
    "Dataset n1 (ds1) is a copy from Cofano et al[6]. Dependencies are read from `requirements.txt` from `\\sbom`. This dataset contains no ground truth, and only `\\sbom` can be used to draw a comparison between our new approach and the other tools. \n",
    "Here, `COM713`, a package specially crafted to avoid detection by regular metadata-only tools has been added in the `requirements.txt` files. This will allow us to test that our AST analyser is immune to parser confusion.\n",
    "\n",
    "### 2.2 Dataset n°2\n",
    "Dataset n2 (ds2) is a copy from Jia et al's[7] dataset. `\\deptree_gt` contains the ground truth as json files for each package to compare with the other tools real performance in `\\sbom`. \n",
    "\n",
    "### 2.3 Preprocessing and merging\n",
    "These two datasets have been parsed and merged externally; the process required a cyclonedx library that couldnt be attached to this project. However, if curious as to how it worked, you can peek into the `merge.py` file and see how it got done. It required serialising our dataclasses from above into json files. By merging is meant combining all previous relevant dataset files into one, but we keep both datasets distinct for practical reasons. Deserialising those datasets meant implementing `from_dict` functions as to retroactively convert all nested objects into their original types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c46c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset1: Dataset\n",
    "dataset2: Dataset\n",
    "\n",
    "with open(\"merged_ds1.json\", \"r\") as dataset_file:\n",
    "    dataset1 =  Dataset.from_dict(json.loads(dataset_file.read())) \n",
    "\n",
    "with open(\"merged_ds2.json\", \"r\") as dataset_file:\n",
    "    dataset2 =  Dataset.from_dict(json.loads(dataset_file.read())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345897cd",
   "metadata": {},
   "source": [
    "## 3. AST Algorithm\n",
    "Our AST algorithm follows this logic: \n",
    "\n",
    "```\n",
    "                                                                               ┌───────────────────┐     \n",
    "                                                                               │                   │     \n",
    "                                                                ┌────────┐     │                   │     \n",
    "                                                                │        │     │ Print the SBOM    │     \n",
    "                                                                │ Start ◄┼─────┼ and export it as a│     \n",
    "                                                                │        │     │ file             ◄┼┐    \n",
    "                                                                └────────┘     │                   ││    \n",
    "                                                                               └───────────────────┘│    \n",
    "                                                                                                    │    \n",
    "                                                                                                   No    \n",
    "                                                                                                    │    \n",
    "                                                                                                    │    \n",
    "                                                                                                 xxx│x   \n",
    "                                                                             xxxxx              xx   xx  \n",
    "                  ┌────────────────┐        ┌────────────────────┐          xx   xx            xx     xx \n",
    "                  │                │        │                    │         xx     xx          xx Any   xx\n",
    "┌────────┐        │                │        │ Look through the   │        xx Any   xx         x  .py    x\n",
    "│        │        │  Read source   │        │ AST for import     │        x  import ───No─────►x left? xx\n",
    "│ Start ─┼────────┼► file (.py),   ┼────────► statements         ┼────────►x left? xx          xx     xx \n",
    "│        │        │  build the AST │        │                    │         xx     xx            xx   xx  \n",
    "└────────┘        │                │        │                    │          xx   xx              xx xx   \n",
    "                  └──────▲─────────┘        └────────▲───────────┘           xx xx                │xx    \n",
    "                         │                           │                        x│x                 │      \n",
    "                         │                           │                         │                  │      \n",
    "                         │                           │                         │                  │      \n",
    "                         │                           │                        Yes                Yes     \n",
    "                         │                  ┌────────┼───────────┐             │                  │      \n",
    "                         │                  │                    │             │                  │      \n",
    "                         │                  │  Add to graph      │             │                  │      \n",
    "                         │                  │                   ◄│─────────────┘                  │      \n",
    "                         │                  │                    │                                │      \n",
    "                         │                  │                    │      ┌────────────────────┐    │      \n",
    "                         │                  └────────────────────┘      │                    │    │      \n",
    "                         │                                              │  Move to next      │    │      \n",
    "                         └──────────────────────────────────────────────│  source file,      │────┘      \n",
    "                                                                        │  mark file as read │           \n",
    "                                                                        │                    │           \n",
    "                                                                        └────────────────────┘           \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "128bb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "from importlib.util import find_spec\n",
    "from importlib.metadata import packages_distributions\n",
    "\n",
    "class PackageAnalyser(ast.NodeVisitor):\n",
    "    \"\"\"\n",
    "    Parses the package's sourcecode, build an abstract syntax tree, and from this, identifies the imports \n",
    "    that will allow to dive deeper into each imported package. This process is what generates our dependency graph, \n",
    "    where each Vertex or Package will be an entry in our SBOM.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_path: str, root: str):\n",
    "        self.source_path = source_path\n",
    "        self.root = root\n",
    "        self.graph = DependencyGraph()\n",
    "        self.current_file = \"\"\n",
    "        self.distribution_packages = packages_distributions()\n",
    "        self._visited_nodes = {}\n",
    "        self.current_level = 0\n",
    "\n",
    "        if os.path.exists(self.source_path):\n",
    "            if (os.path.isfile(self.source_path)) and source_path.endswith(\".py\"):\n",
    "                #self.graph.insert_package(root_package_name)\n",
    "                self.current_file = self.source_path\n",
    "            elif (os.path.isdir(self.source_path)):\n",
    "                try:\n",
    "                    self.current_file = [file for file in os.listdir(self.source_path) if file.endswith(\".py\")][0]\n",
    "                except:\n",
    "                    print(\"Cannot find py for\" + source_path)\n",
    "                    exit(1)\n",
    "\n",
    "    \"\"\"\n",
    "    Each visit_x function is called each time a node of a node of our abstract syntax tree is visited\n",
    "    _Module -> is called each time a new module is visited\n",
    "    _Import -> is called each time a new import is visited\n",
    "    _ImportFrom -> is called each time a a new from ... import is visited \n",
    "    \"\"\"\n",
    "    def visit_Module(self, node: ast.Module) -> None:\n",
    "        self._visited_nodes[self.current_file] = self.current_level\n",
    "        path = Path(self.current_file)\n",
    "\n",
    "        # we avoid the __init__ and whatnot, and we try to guess the package name based on the parent directory\n",
    "        if (path.stem.startswith(\"__\")):\n",
    "            if (path.parent.is_dir() and path.parent.stem in self.distribution_packages):\n",
    "                self.current_package = self.graph.insert_package(path.parent.stem, self.current_level)\n",
    "        #else:\n",
    "        \n",
    "        #self._parse_and_visit(self.current_file) # This makes it a recursive process!\n",
    "            #package_path = self._find_package_path(package_name)\n",
    "        self.generic_visit(node)\n",
    "    \n",
    "\n",
    "    # O(n!) Really bad, _parse_and_visit will both read, build the AST tree, and look for \n",
    "    # further files to parse by calling itself during the \"visiting\" of the tree\n",
    "    def visit_Import(self, node: ast.Import) -> None:\n",
    "        package_name = node.names[0].name # the first name works\n",
    "\n",
    "        if (not self._is_separate_package(package_name)):\n",
    "            return\n",
    "        \n",
    "        package_path = self._find_package_path(package_name)\n",
    "        self._parse_and_visit(package_path)\n",
    "        \n",
    "        # ultimately, we only want to add to our graph the distribution packages\n",
    "        if (package_name in self.distribution_packages and Package(package_name) not in self.graph.packages):\n",
    "            package_path = self._find_package_path(package_name)\n",
    "            self._parse_and_visit(package_path) # This makes it a recursive process!\n",
    "            new_package = self.graph.insert_package(package_name, self.current_level)\n",
    "            self.graph.insert_importstatement(\n",
    "                self.current_package, \n",
    "                new_package)\n",
    "            self.current_package = new_package\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    # O(n!) Really bad, _parse_and_visit will both read, build the AST tree, and look for \n",
    "    # further files to parse by calling itself during the \"visiting\" of the tree\n",
    "    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:\n",
    "        package_name = node.module or \"\" # we want the x from 'from x import y', not the y\n",
    "\n",
    "        if (not self._is_separate_package(package_name)):\n",
    "            return\n",
    "        \n",
    "        # ultimately, we only want to add to our graph the distribution packages\n",
    "        if (package_name in self.distribution_packages and Package(package_name) not in self.graph.packages):\n",
    "            package_path = self._find_package_path(package_name)\n",
    "            self._parse_and_visit(package_path) # This makes it a recursive process!\n",
    "            new_package = self.graph.insert_package(package_name, self.current_level)\n",
    "            self.graph.insert_importstatement(\n",
    "                self.current_package, \n",
    "                new_package)\n",
    "            self.current_package = new_package\n",
    "\n",
    "            self.generic_visit(node)\n",
    "\n",
    "    # O(1) nothing too special\n",
    "    def _is_separate_package(self, node_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Distinguish between just regular modules, proper modules from separate installed packages\n",
    "        and stdlib packages\n",
    "        \"\"\"\n",
    "        if node_name == '':\n",
    "            return False\n",
    "\n",
    "        if node_name in sys.stdlib_module_names:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            spec = find_spec(node_name)\n",
    "            if spec is None or spec.submodule_search_locations is None:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # O(c), constant time, nothing special\n",
    "    def _find_package_path(self, node_name: str) -> str:\n",
    "        spec = find_spec(node_name)       \n",
    "        if (spec is None or spec.submodule_search_locations is None or spec.origin is None):\n",
    "            raise FileNotFoundError(\"Couldn't find the package's og source\")\n",
    "        else:\n",
    "            return spec.origin\n",
    "        \n",
    "    # O(n!) The root of all evil in our programme. \n",
    "    def _parse_and_visit(self, file_path: str, reset_level: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Read the source, parse it to further build the AST, then explore its edges to discover other packages\n",
    "        \"\"\"\n",
    "        if not file_path.endswith(\".py\"):\n",
    "            return \n",
    "        \n",
    "        if file_path in self._visited_nodes: # a look up is constant thx to the fact that its a hashmap!\n",
    "            return \n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\") as source_file:\n",
    "                if (reset_level is True):\n",
    "                    self.current_level = 0\n",
    "                    package_name = Path(self.current_file).stem\n",
    "                    self.current_package = self.graph.insert_package(package_name, self.current_level)\n",
    "                else:\n",
    "                    self.current_level += 1\n",
    "                code = source_file.read()\n",
    "                tree = ast.parse(code)\n",
    "                self.current_file = file_path\n",
    "                self._visited_nodes[file_path] = self.current_level\n",
    "                self.visit(tree)\n",
    "        except:\n",
    "            return\n",
    "            \n",
    "    \n",
    "    # O(n^2) => quadratic due to the looping through each dirs and files\n",
    "    def analyse(self) -> None:\n",
    "        \"\"\"\n",
    "        Will parse the package's source code and build an AST from it, where every node from the tree will be visited\n",
    "        \"\"\"\n",
    "        if (os.path.isfile(self.source_path)):\n",
    "            self._parse_and_visit(self.source_path, True)\n",
    "        elif (os.path.isdir(self.source_path)):\n",
    "            for root, _, files in os.walk(self.source_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    self._parse_and_visit(file_path, True)\n",
    "    \n",
    "    def print_packages(self) -> None:\n",
    "        print(\"Project: \" + self.root)\n",
    "                \n",
    "        for package in self.graph.packages:\n",
    "            #if (package.level > 0):\n",
    "            print(package.name)\n",
    "        print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0f1fe",
   "metadata": {},
   "source": [
    "Here is displayed the core packages of each project. \n",
    "`com713` is the most interesting one; this one was added in the dataset n°1 to confuse parsers\n",
    "We see that our algorithm is thus perfectly capable of identifying distribution packages, even when \"invisible\" from the metadata readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3a4aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: pip-hatchling\n",
      "numpy\n",
      "main\n",
      "com713\n",
      "-------------------------\n",
      "Project: pip-pdm\n",
      "numpy\n",
      "main\n",
      "com713\n",
      "-------------------------\n",
      "Project: pip-setuptools\n",
      "numpy\n",
      "main\n",
      "com713\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for package, analysis in dataset1.package_analyses.items():\n",
    "    sys.path.insert(0, os.path.abspath(analysis.packages_path)) # makes the main package's own packages available and thus importable\n",
    "    analyser = PackageAnalyser(source_path=analysis.source_path, root=package)\n",
    "    analyser.analyse()\n",
    "    analyser.print_packages()\n",
    "    sys.path.pop(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00052fa1",
   "metadata": {},
   "source": [
    "For our second dataset, we can see that our algorithm is capable of iterating through each distribution package \n",
    "instead of simply relying on `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca20f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: apprise\n",
      "apprise\n",
      "-------------------------\n",
      "Project: django-rest-framework\n",
      "documentation\n",
      "routers\n",
      "generators\n",
      "inspectors\n",
      "validators\n",
      "html\n",
      "representation\n",
      "drf_create_token\n",
      "0001_initial\n",
      "apps\n",
      "serializer_helpers\n",
      "django\n",
      "viewsets\n",
      "json\n",
      "openapi\n",
      "mediatypes\n",
      "serializers\n",
      "filters\n",
      "authentication\n",
      "status\n",
      "humanize_datetime\n",
      "0003_tokenproxy\n",
      "charset_normalizer\n",
      "metadata\n",
      "coreschema\n",
      "generics\n",
      "cryptography\n",
      "uritemplate\n",
      "request\n",
      "markdown\n",
      "inflection\n",
      "yaml\n",
      "renderers\n",
      "versioning\n",
      "0002_auto_20160226_1747\n",
      "utils\n",
      "field_mapping\n",
      "throttling\n",
      "exceptions\n",
      "pkg_resources\n",
      "generateschema\n",
      "requests\n",
      "timezone\n",
      "formatting\n",
      "views\n",
      "pygments\n",
      "response\n",
      "mixins\n",
      "permissions\n",
      "relations\n",
      "__init__\n",
      "settings\n",
      "parsers\n",
      "0004_alter_tokenproxy_options\n",
      "pagination\n",
      "negotiation\n",
      "rest_framework\n",
      "breadcrumbs\n",
      "admin\n",
      "model_meta\n",
      "coreapi\n",
      "urls\n",
      "checks\n",
      "urllib3\n",
      "decorators\n",
      "test\n",
      "models\n",
      "encoders\n",
      "urlpatterns\n",
      "reverse\n",
      "-------------------------\n",
      "Project: fastapi\n",
      "templating\n",
      "exception_handlers\n",
      "starlette\n",
      "param_functions\n",
      "http\n",
      "api_key\n",
      "anyio\n",
      "base\n",
      "pydantic\n",
      "websockets\n",
      "pydantic_core\n",
      "wsgi\n",
      "oauth2\n",
      "cli\n",
      "applications\n",
      "utils\n",
      "orjson\n",
      "types\n",
      "gzip\n",
      "datastructures\n",
      "email_validator\n",
      "multipart\n",
      "docs\n",
      "exceptions\n",
      "requests\n",
      "staticfiles\n",
      "params\n",
      "logger\n",
      "__init__\n",
      "trustedhost\n",
      "_compat\n",
      "httpsredirect\n",
      "constants\n",
      "background\n",
      "open_id_connect_url\n",
      "__main__\n",
      "testclient\n",
      "models\n",
      "cors\n",
      "-------------------------\n",
      "Project: impacket\n",
      "msada_guids\n",
      "eap\n",
      "samr\n",
      "ntlm\n",
      "dpapi\n",
      "mssqlrelayclient\n",
      "httprelayserver\n",
      "ldapdomaindump\n",
      "rpcrt\n",
      "even6\n",
      "nmb\n",
      "mimilib\n",
      "dssp\n",
      "nt_errors\n",
      "bkrp\n",
      "tsch\n",
      "https\n",
      "smtp\n",
      "types\n",
      "Dot11Crypto\n",
      "hresult_errors\n",
      "pkg_resources\n",
      "structure\n",
      "six\n",
      "ssl\n",
      "OpenSSL\n",
      "rawrelayserver\n",
      "constants\n",
      "ldap3\n",
      "ldaprelayclient\n",
      "shadow_credentials\n",
      "kpasswd\n",
      "system_errors\n",
      "spnego\n",
      "helper\n",
      "IP6\n",
      "oxabref\n",
      "nspi\n",
      "adcsattack\n",
      "mssqlshell\n",
      "winregistry\n",
      "flask\n",
      "config\n",
      "tcpshell\n",
      "par\n",
      "ldaptypes\n",
      "utils\n",
      "IP6_Address\n",
      "mssql\n",
      "srvs\n",
      "wcfrelayserver\n",
      "crypto\n",
      "rpch\n",
      "vds\n",
      "rpcattack\n",
      "drsuapi\n",
      "logger\n",
      "smb3structs\n",
      "os_ident\n",
      "dcomrt\n",
      "transport\n",
      "uuid\n",
      "pyasn1\n",
      "http\n",
      "nrpc\n",
      "remcomsvc\n",
      "sasec\n",
      "imaprelayclient\n",
      "dpapi_ng\n",
      "wps\n",
      "smbconnection\n",
      "IP6_Extension_Headers\n",
      "scmp\n",
      "dhcpm\n",
      "dcsyncclient\n",
      "ImpactPacket\n",
      "even\n",
      "imaps\n",
      "httprelayclient\n",
      "smbrelayclient\n",
      "targetsutils\n",
      "epm\n",
      "smb\n",
      "ndr\n",
      "lsad\n",
      "lsat\n",
      "tds\n",
      "mgmt\n",
      "comev\n",
      "kerberosv5\n",
      "rpcrelayclient\n",
      "__init__\n",
      "smbrelayserver\n",
      "dot11\n",
      "ICMP6\n",
      "serviceinstall\n",
      "imap\n",
      "NDP\n",
      "oaut\n",
      "pac\n",
      "imapattack\n",
      "dns\n",
      "smbclient\n",
      "rpcdatabase\n",
      "smbattack\n",
      "enum\n",
      "atsvc\n",
      "dtypes\n",
      "gssapi\n",
      "ldapasn1\n",
      "charset_normalizer\n",
      "scmr\n",
      "ImpactDecoder\n",
      "asn1\n",
      "rprn\n",
      "wkst\n",
      "dhcp\n",
      "iphlp\n",
      "smb3\n",
      "ese\n",
      "mssqlattack\n",
      "wmi\n",
      "smtprelayclient\n",
      "ccache\n",
      "dcsyncattack\n",
      "ldap\n",
      "pcapfile\n",
      "mapi_constants\n",
      "pcap_linktypes\n",
      "rrp\n",
      "smbserver\n",
      "httpattack\n",
      "gkdi\n",
      "Dot11KeyManager\n",
      "tsts\n",
      "cdp\n",
      "keytab\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:2151: SyntaxWarning: invalid escape sequence '\\s'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: InstaPy\n",
      "telegram_util\n",
      "chardet\n",
      "pyvirtualdisplay\n",
      "event\n",
      "clarifai_util\n",
      "time_util\n",
      "xpath\n",
      "webdriverdownloader\n",
      "selenium\n",
      "charset_normalizer\n",
      "regex\n",
      "meaningcloud\n",
      "relationship_tools\n",
      "exceptions\n",
      "commenters_util\n",
      "pkg_resources\n",
      "browser\n",
      "apidisplaypurposes\n",
      "requests\n",
      "file_manager\n",
      "plyer\n",
      "unfollow_util\n",
      "story_util\n",
      "print_log_writer\n",
      "settings\n",
      "__init__\n",
      "follow_util\n",
      "bs4\n",
      "login_util\n",
      "constants\n",
      "database_engine\n",
      "like_util\n",
      "monkey_patcher\n",
      "emoji\n",
      "urllib3\n",
      "pods_util\n",
      "feed_util\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib.util>:91: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.4.0)\n",
      "<frozen importlib.util>:91: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: keras\n",
      "mnist\n",
      "regression_metrics\n",
      "torch_data_loader_adapter_test\n",
      "serialization_lib\n",
      "saving_api\n",
      "einsum_dense\n",
      "keras_tensor\n",
      "linalg\n",
      "adamax_test\n",
      "absl\n",
      "json_utils_test\n",
      "adamax\n",
      "numerical_utils\n",
      "object_registration_test\n",
      "conv_lstm\n",
      "cloning_test\n",
      "random_initializers\n",
      "ftrl\n",
      "remote_monitor_test\n",
      "lion\n",
      "stateless_scope_test\n",
      "callback\n",
      "average_pooling1d\n",
      "optimizer\n",
      "torch_optimizer\n",
      "constraints\n",
      "global_average_pooling1d\n",
      "file_utils_test\n",
      "dtype_policy_map\n",
      "random_test\n",
      "learning_rate_schedule_test\n",
      "feature_space_test\n",
      "rmsprop\n",
      "adamw_test\n",
      "functional\n",
      "h5py\n",
      "normalization_test\n",
      "saving_lib\n",
      "up_sampling3d\n",
      "io_utils\n",
      "compute_output_spec_test\n",
      "cropping2d_test\n",
      "image_test\n",
      "layer_normalization\n",
      "integer_lookup\n",
      "inception_v3\n",
      "random_crop\n",
      "lamb_test\n",
      "accuracy_metrics\n",
      "swap_ema_weights\n",
      "embedding\n",
      "backend_utils_test\n",
      "json_utils\n",
      "conv_lstm3d\n",
      "learning_rate_schedule\n",
      "variables\n",
      "string_lookup_test\n",
      "tf_dataset_adapter\n",
      "nn\n",
      "zero_padding3d_test\n",
      "regularizers\n",
      "zero_padding2d_test\n",
      "export_lib_test\n",
      "generator_data_adapter_test\n",
      "torch_nadam\n",
      "random_brightness\n",
      "random_contrast\n",
      "resizing\n",
      "boston_housing\n",
      "sequential\n",
      "exports\n",
      "spatial_dropout_test\n",
      "dropout_rnn_cell\n",
      "imagenet_utils\n",
      "average_pooling_test\n",
      "progbar_logger\n",
      "average_pooling2d\n",
      "core\n",
      "einsum_dense_test\n",
      "urllib3\n",
      "stateless_scope\n",
      "additive_attention\n",
      "cloning\n",
      "global_average_pooling_test\n",
      "imdb\n",
      "conv_lstm1d\n",
      "adamw\n",
      "spectral_normalization_test\n",
      "fashion_mnist\n",
      "lamb\n",
      "distribution_lib_test\n",
      "inception_resnet_v2\n",
      "up_sampling3d_test\n",
      "terminate_on_nan_test\n",
      "global_max_pooling1d\n",
      "hashing_test\n",
      "audio_preprocessing\n",
      "sequence_utils_test\n",
      "permute_test\n",
      "batch_normalization\n",
      "bidirectional_test\n",
      "separable_conv_test\n",
      "jax_layer\n",
      "dtype_utils\n",
      "operation\n",
      "text\n",
      "generator_data_adapter\n",
      "optimizer_sparse_test\n",
      "data_adapter\n",
      "lion_test\n",
      "unit_normalization_test\n",
      "global_max_pooling3d\n",
      "cropping3d_test\n",
      "base_global_pooling\n",
      "loss_test\n",
      "center_crop_test\n",
      "nadam\n",
      "symbolic_arguments_test\n",
      "adagrad_test\n",
      "sympy\n",
      "python_utils\n",
      "torch\n",
      "adafactor\n",
      "object_registration\n",
      "pytest\n",
      "namex\n",
      "maximum\n",
      "random_rotation_test\n",
      "minimum\n",
      "ml_dtypes\n",
      "argument_validation\n",
      "identity_test\n",
      "distribute_test\n",
      "backup_and_restore\n",
      "saved_model_test\n",
      "f_score_metrics_test\n",
      "nadam_test\n",
      "sgd\n",
      "conv1d_transpose\n",
      "subtract\n",
      "pandas\n",
      "dtype_policy_test\n",
      "spatial_dropout\n",
      "dropout\n",
      "base_conv_transpose\n",
      "torch_adamax\n",
      "flax\n",
      "hashing\n",
      "layer_test\n",
      "torch_adadelta\n",
      "dropout_rnn_cell_test\n",
      "probabilistic_metrics\n",
      "lambda_layer\n",
      "base_optimizer\n",
      "serialization_lib_test\n",
      "conv3d_transpose\n",
      "hashed_crossing\n",
      "tf_keras\n",
      "conv_transpose_test\n",
      "zero_padding1d\n",
      "compile_utils\n",
      "relu_test\n",
      "compile_utils_test\n",
      "cifar\n",
      "rich\n",
      "global_max_pooling_test\n",
      "math_test\n",
      "variable_mapping_test\n",
      "variables_test\n",
      "layer_normalization_test\n",
      "masking_test\n",
      "random\n",
      "activity_regularization\n",
      "PIL\n",
      "reshape_test\n",
      "rnn\n",
      "input_layer_test\n",
      "optree\n",
      "model_visualization\n",
      "identity\n",
      "conv_test\n",
      "index_lookup\n",
      "torch_parallel_optimizer\n",
      "keras\n",
      "up_sampling2d\n",
      "layers\n",
      "audio_dataset_utils\n",
      "variable_mapping\n",
      "code_stats\n",
      "loss_scale_optimizer\n",
      "grouped_query_attention\n",
      "alpha_dropout\n",
      "separable_conv2d\n",
      "tensorflow\n",
      "backend\n",
      "base_merge\n",
      "max_pooling_test\n",
      "metrics_utils\n",
      "audio_preprocessing_test\n",
      "image\n",
      "feature_space\n",
      "__init__\n",
      "probabilistic_metrics_test\n",
      "torch_adamw\n",
      "conv_lstm_test\n",
      "concatenate\n",
      "category_encoding_test\n",
      "adadelta\n",
      "max_pooling3d\n",
      "spectral_normalization\n",
      "gaussian_dropout_test\n",
      "repeat_vector_test\n",
      "linalg_test\n",
      "IPython\n",
      "depthwise_conv1d\n",
      "core_test\n",
      "dot\n",
      "symbolic_arguments\n",
      "base_conv\n",
      "conv_lstm2d_test\n",
      "version\n",
      "rmsprop_test\n",
      "function_test\n",
      "saving_utils\n",
      "callback_test\n",
      "adam\n",
      "image_dataset_utils_test\n",
      "tf_utils\n",
      "cifar10\n",
      "loss\n",
      "regression_metrics_test\n",
      "test_utils\n",
      "function\n",
      "separable_conv1d\n",
      "additive_attention_test\n",
      "code_stats_test\n",
      "dtype_policy\n",
      "gaussian_noise_test\n",
      "symbolic_scope\n",
      "text_vectorization\n",
      "history\n",
      "base_pooling\n",
      "data_adapter_utils\n",
      "iou_metrics\n",
      "category_encoding\n",
      "bidirectional\n",
      "trainer_test\n",
      "group_normalization\n",
      "conv_lstm1d_test\n",
      "efficientnet_v2\n",
      "image_dataset_utils\n",
      "text_dataset_utils_test\n",
      "confusion_metrics\n",
      "relu\n",
      "grouped_query_attention_test\n",
      "optimizer_test\n",
      "learning_rate_scheduler\n",
      "text_vectorization_test\n",
      "losses\n",
      "jax_utils\n",
      "python_utils_test\n",
      "quantizers\n",
      "random_translation_test\n",
      "timeseries_dataset_utils\n",
      "torch_utils\n",
      "index_lookup_test\n",
      "sparse\n",
      "permute\n",
      "nn_test\n",
      "integer_lookup_test\n",
      "flatten\n",
      "random_flip\n",
      "conv1d\n",
      "learning_rate_scheduler_test\n",
      "cropping3d\n",
      "summary_utils_test\n",
      "sequential_test\n",
      "saving_lib_test\n",
      "base_depthwise_conv\n",
      "text_dataset_utils\n",
      "hinge_metrics\n",
      "simple_rnn_test\n",
      "trackable\n",
      "prelu_test\n",
      "rnn_test\n",
      "adafactor_test\n",
      "dense\n",
      "center_crop\n",
      "discretization_test\n",
      "timeseries_dataset_utils_test\n",
      "operation_utils\n",
      "conv_lstm2d\n",
      "device_scope_test\n",
      "backup_and_restore_test\n",
      "name_scope_test\n",
      "dtype_policy_map_test\n",
      "array_slicing\n",
      "operation_test\n",
      "depthwise_conv_test\n",
      "metric_test\n",
      "early_stopping_test\n",
      "vgg19\n",
      "swap_ema_weights_test\n",
      "functional_test\n",
      "adagrad\n",
      "masking\n",
      "lstm_test\n",
      "gaussian_noise\n",
      "math\n",
      "random_zoom_test\n",
      "vgg16\n",
      "export_lib\n",
      "time_distributed_test\n",
      "node\n",
      "constant_initializers_test\n",
      "global_max_pooling2d\n",
      "numpy_test\n",
      "hashed_crossing_test\n",
      "zero_padding3d\n",
      "discretization\n",
      "normalization\n",
      "dropout_test\n",
      "tf_dataset_adapter_test\n",
      "image_utils\n",
      "iou_metrics_test\n",
      "naming_test\n",
      "_pytest\n",
      "operation_utils_test\n",
      "alpha_dropout_test\n",
      "activation\n",
      "cifar100\n",
      "reduction_metrics\n",
      "resizing_test\n",
      "model_checkpoint\n",
      "wrapper\n",
      "tracking\n",
      "sequence_utils\n",
      "attention_test\n",
      "random_crop_test\n",
      "dtype_utils_test\n",
      "model\n",
      "dtypes\n",
      "keras_saveable\n",
      "seed_generator\n",
      "mpmath\n",
      "losses_test\n",
      "serialization\n",
      "nasnet\n",
      "random_rotation\n",
      "saving_api_test\n",
      "random_translation\n",
      "dtypes_test\n",
      "reshape\n",
      "conv2d\n",
      "tf_data_layer\n",
      "global_state\n",
      "tensorboard_test\n",
      "softmax_test\n",
      "xception\n",
      "rescaling\n",
      "densenet\n",
      "adadelta_test\n",
      "merging_test\n",
      "gaussian_dropout\n",
      "activation_test\n",
      "lambda_callback\n",
      "elu_test\n",
      "traceback_utils\n",
      "adam_test\n",
      "input_layer\n",
      "random_contrast_test\n",
      "flatten_test\n",
      "tensorflow_io_gcs_filesystem\n",
      "node_test\n",
      "constant_initializers\n",
      "zero_padding1d_test\n",
      "dmtree_impl\n",
      "softmax\n",
      "torch_sgd\n",
      "name_scope\n",
      "gru\n",
      "lstm\n",
      "dense_test\n",
      "random_brightness_test\n",
      "loss_scale_optimizer_test\n",
      "distribution_lib\n",
      "epoch_iterator\n",
      "global_average_pooling2d\n",
      "prelu\n",
      "string_lookup\n",
      "numpy\n",
      "sequence\n",
      "elu\n",
      "multiply\n",
      "gru_test\n",
      "mobilenet_v2\n",
      "rng_utils_test\n",
      "io_utils_test\n",
      "zero_padding2d\n",
      "early_stopping\n",
      "attention\n",
      "config\n",
      "stacked_rnn_cells\n",
      "time_distributed\n",
      "psutil\n",
      "depthwise_conv2d\n",
      "lambda_layer_test\n",
      "tree_api\n",
      "conv_lstm3d_test\n",
      "up_sampling2d_test\n",
      "conv3d\n",
      "torch_lion\n",
      "numerical_utils_test\n",
      "conv2d_transpose\n",
      "requests\n",
      "cropping1d_test\n",
      "tree_test\n",
      "group_normalization_test\n",
      "scipy\n",
      "saving_options\n",
      "sgd_test\n",
      "up_sampling1d_test\n",
      "torch_utils_test\n",
      "cropping1d\n",
      "csv_logger_test\n",
      "quantizers_test\n",
      "hinge_metrics_test\n",
      "add\n",
      "average_pooling3d\n",
      "random_zoom\n",
      "random_initializers_test\n",
      "model_test\n",
      "dataset_utils\n",
      "max_pooling1d\n",
      "average\n",
      "module_utils\n",
      "torch_rmsprop\n",
      "legacy_h5_format\n",
      "test_utils_test\n",
      "array_data_adapter\n",
      "constraints_test\n",
      "csv_logger\n",
      "metric\n",
      "terminate_on_nan\n",
      "tree\n",
      "efficientnet\n",
      "base_separable_conv\n",
      "batch_normalization_test\n",
      "callback_list\n",
      "simple_rnn\n",
      "torch_adagrad\n",
      "input_spec\n",
      "leaky_relu\n",
      "multi_head_attention_test\n",
      "max_pooling2d\n",
      "accuracy_metrics_test\n",
      "jax\n",
      "leaky_relu_test\n",
      "activations\n",
      "py_dataset_adapter_test\n",
      "california_housing\n",
      "unit_normalization\n",
      "wrapper_test\n",
      "test_case\n",
      "resnet\n",
      "embedding_test\n",
      "resnet_v2\n",
      "convnext\n",
      "torch_adam\n",
      "activity_regularization_test\n",
      "up_sampling1d\n",
      "regularizers_test\n",
      "global_average_pooling3d\n",
      "epoch_iterator_test\n",
      "multi_head_attention\n",
      "dataset_utils_test\n",
      "mobilenet\n",
      "layer\n",
      "repeat_vector\n",
      "charset_normalizer\n",
      "audio_dataset_utils_test\n",
      "f_score_metrics\n",
      "ftrl_test\n",
      "naming\n",
      "trainer\n",
      "backend_utils\n",
      "reduce_lr_on_plateau\n",
      "reduction_metrics_test\n",
      "tracking_test\n",
      "rng_utils\n",
      "torch_data_loader_adapter\n",
      "tensorboard\n",
      "legacy_h5_format_test\n",
      "rescaling_test\n",
      "symbolic_scope_test\n",
      "global_state_test\n",
      "stacked_rnn_cells_test\n",
      "reduce_lr_on_plateau_test\n",
      "initializer\n",
      "mobilenet_v3\n",
      "reuters\n",
      "random_flip_test\n",
      "cropping2d\n",
      "seed_generator_test\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:121: SyntaxWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: scancode-toolkit\n",
      "boolean\n",
      "distro\n",
      "validators\n",
      "nuget\n",
      "plugin_license_policy\n",
      "seq\n",
      "cache\n",
      "match\n",
      "windows\n",
      "pymaven\n",
      "help\n",
      "match_hash\n",
      "_make\n",
      "readme\n",
      "cryptography\n",
      "rpm\n",
      "plugin_ignore_copyrights\n",
      "toml\n",
      "debian_copyright\n",
      "spec\n",
      "match_set\n",
      "misc\n",
      "six\n",
      "fingerprints\n",
      "go_mod\n",
      "pdf\n",
      "licenses_reference\n",
      "match_aho\n",
      "facet\n",
      "rpm_installed\n",
      "todo\n",
      "lxml\n",
      "click\n",
      "javaproperties\n",
      "analysis\n",
      "models\n",
      "golang\n",
      "finder\n",
      "pypi_setup_py\n",
      "swift\n",
      "nevra\n",
      "interrupt\n",
      "plugin_url\n",
      "chardet\n",
      "classify\n",
      "_cmp\n",
      "filters\n",
      "tracing\n",
      "cocoapods\n",
      "debian\n",
      "cli_test_utils\n",
      "msi\n",
      "licensedcode_test_utils\n",
      "finder_data\n",
      "output_yaml\n",
      "legal\n",
      "rubygems\n",
      "utils\n",
      "output_jsonlines\n",
      "plugin_copyright\n",
      "bashlex\n",
      "packageurl\n",
      "jar_manifest\n",
      "exceptions\n",
      "phpcomposer\n",
      "match_spdx_lid\n",
      "classify_plugin\n",
      "groovy_lexer\n",
      "requests\n",
      "plugincode\n",
      "pygments\n",
      "score\n",
      "_compat\n",
      "index\n",
      "summarizer\n",
      "fasteners\n",
      "detection\n",
      "plugin_package\n",
      "ftfy\n",
      "haxe\n",
      "parameter_expansion\n",
      "commoncode\n",
      "jinja2\n",
      "frontmatter\n",
      "reindex\n",
      "jsonstreams\n",
      "tallies\n",
      "godeps\n",
      "win_pe\n",
      "setters\n",
      "chef\n",
      "_version_info\n",
      "copyrights_hint\n",
      "plugin_license\n",
      "output_spdx\n",
      "dparse2\n",
      "recognize\n",
      "cran\n",
      "_config\n",
      "license_db\n",
      "plugin_email\n",
      "gemfile_lock\n",
      "generated\n",
      "scancode_config\n",
      "__init__\n",
      "output_debian\n",
      "converters\n",
      "plugin_ignore\n",
      "tokenize\n",
      "text_unidecode\n",
      "conan\n",
      "plugin_only_findings\n",
      "pool\n",
      "markup\n",
      "plugin_mark_source\n",
      "urllib3\n",
      "match_unknown\n",
      "plugin_filter_clues\n",
      "sfdb\n",
      "api\n",
      "_next_gen\n",
      "alpine\n",
      "attr\n",
      "win_reg\n",
      "bower\n",
      "license_expression\n",
      "spans\n",
      "dmp\n",
      "pygmars\n",
      "pkginfo2\n",
      "stopwords\n",
      "typecode\n",
      "packvers\n",
      "charset_normalizer\n",
      "opam\n",
      "regen_package_docs\n",
      "cli\n",
      "plugin_consolidate\n",
      "match_seq\n",
      "strings2\n",
      "pubspec\n",
      "importlib_metadata\n",
      "pluggy\n",
      "licensing\n",
      "attrs\n",
      "freebsd\n",
      "conda\n",
      "_funcs\n",
      "strings\n",
      "plugin_info\n",
      "build\n",
      "npm\n",
      "-------------------------\n",
      "Cannot find py for.\\ds2\\packages\\ydata-profiling\\src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib.util>:91: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: ydata-profiling\n",
      "common\n",
      "compat\n",
      "render_categorical\n",
      "timeseries_index\n",
      "imbalance_pandas\n",
      "variable_info\n",
      "expectation_algorithms\n",
      "render_generic\n",
      "typeguard\n",
      "missing_spark\n",
      "summary_spark\n",
      "timeseries_index_spark\n",
      "multimethod\n",
      "render_real\n",
      "duplicate\n",
      "pairwise\n",
      "describe_categorical_spark\n",
      "dropdown\n",
      "pkg_resources\n",
      "matplotlib\n",
      "describe\n",
      "dataframe_pandas\n",
      "render_boolean\n",
      "numpy\n",
      "missing_pandas\n",
      "describe_generic_pandas\n",
      "pandas\n",
      "alerts\n",
      "progress_bar\n",
      "describe_boolean_spark\n",
      "packaging\n",
      "render_file\n",
      "describe_supported_pandas\n",
      "render_image\n",
      "timeseries_index_pandas\n",
      "describe_generic_spark\n",
      "serialize_report\n",
      "frequency_table\n",
      "paths\n",
      "markupsafe\n",
      "yaml\n",
      "utils\n",
      "duplicates_spark\n",
      "render_count\n",
      "sample\n",
      "imghdr_patch\n",
      "frequency_table_small\n",
      "describe_numeric_spark\n",
      "requests\n",
      "logger\n",
      "scipy\n",
      "render_complex\n",
      "renderable\n",
      "summarizer\n",
      "describe_url_pandas\n",
      "describe_text_pandas\n",
      "jinja2\n",
      "describe_supported_spark\n",
      "describe_boolean_pandas\n",
      "table_pandas\n",
      "frequency_table_utils\n",
      "PIL\n",
      "html\n",
      "table_spark\n",
      "describe_path_pandas\n",
      "duplicates_pandas\n",
      "flavours\n",
      "missing\n",
      "describe_timeseries_pandas\n",
      "correlation_table\n",
      "notebook\n",
      "render_timeseries\n",
      "describe_text_spark\n",
      "dacite\n",
      "describe_categorical_pandas\n",
      "render_date\n",
      "overview\n",
      "report\n",
      "\n",
      "describe_counts_pandas\n",
      "certifi\n",
      "typeset\n",
      "image\n",
      "describe_counts_spark\n",
      "toggle_button\n",
      "__init__\n",
      "networkx\n",
      "render_path\n",
      "description\n",
      "describe_date_spark\n",
      "imagehash\n",
      "pywt\n",
      "dataframe_spark\n",
      "typeset_relations\n",
      "urllib3\n",
      "table\n",
      "describe_numeric_pandas\n",
      "visions\n",
      "pandas_decorator\n",
      "sample_pandas\n",
      "root\n",
      "tqdm\n",
      "variable\n",
      "console\n",
      "summary_algorithms\n",
      "item_renderer\n",
      "versions\n",
      "render_common\n",
      "describe_file_pandas\n",
      "charset_normalizer\n",
      "summary_pandas\n",
      "wordcloud\n",
      "version\n",
      "render_text\n",
      "render_url\n",
      "container\n",
      "describe_date_pandas\n",
      "collapse\n",
      "phik\n",
      "dataframe\n",
      "utils_pandas\n",
      "discretize_pandas\n",
      "duplicates\n",
      "sample_spark\n",
      "seaborn\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for package, analysis in dataset2.package_analyses.items():\n",
    "    sys.path.insert(0, os.path.abspath(analysis.packages_path)) # makes the main package's own packages available and thus importable\n",
    "    analyser = PackageAnalyser(source_path=analysis.source_path, root=package)\n",
    "    analyser.analyse()\n",
    "    analyser.print_packages()\n",
    "    sys.path.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041fae85",
   "metadata": {},
   "source": [
    "## 4. Time complexity and limitations\n",
    "Whilst the data structure (an edge-set) has for upper bound a linear time O(n), \n",
    "the mere fact we have to go through each source file, parse the source code, build our tree, \n",
    "and parse other imported files makes our algorithm reach the worst case possible: O(n!)\n",
    "Despite, Sedgewick et Wayne's guide on algorithms[5], where they suggest looping rather than recursing, no functioning looping-approach has been found; a possible one could have been to start from the installed packages and then look for the imports of the same packages in our source code, but that implies possibly heavier FS-interactions.\n",
    "\n",
    "Also, due to time constraints, a further analysis and a comparison between the tools of dataset 2 and the performance of our own implementation has not been done. For now, it can only be noticed that the algorithm is capable of identifying \n",
    "transitive packages, as in, sub packages from other packages, but no metric such as recall or precision could be computed yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9bf21",
   "metadata": {},
   "source": [
    "## AI-Use Compliance\n",
    "ChatGPT, in the present assignment, has been used exclusively for three general tasks: 1. search for specific resources, 2. provide general explanations 3. help diagnose issues or help with debugging. In no case has it been used to generate any part of this document.\n",
    "\n",
    "How could I make the distinction between a module and a package?\n",
    "[a] https://chatgpt.com/share/695d993b-95e4-8008-9fd0-306ad8caed06\n",
    "\n",
    "is there a way to distinguish between python default packages (like os, sys) and those installed from pip, or another source?\n",
    "[b] https://chatgpt.com/share/695d9967-fdec-8008-8425-193decc0c476\n",
    "\n",
    "can you recommend me a book that covers code modelling? as in, for each instruction, how does it influence big O etc.\n",
    "[c] https://chatgpt.com/share/695d9985-cc94-8008-820f-5521bf31ffa8\n",
    "\n",
    "Here, I've noticed that only the first objects (dataset1 and dataset2) are of the right type (Dataset), how can I make sure all sub variables are of the right type?: ...\n",
    "[d] https://chatgpt.com/share/695d99a4-7870-8008-a90e-3ce5295f6956\n",
    "\n",
    "can you recommend me a book that covers code modelling? as in, for each instruction, how does it influence big O etc.\n",
    "[e] https://chatgpt.com/share/695d9985-cc94-8008-820f-5521bf31ffa8\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "[1] J. Tellnes, « Dependencies: No Software is an Island », Master thesis, The University of Bergen, 2013. Available on: https://bora.uib.no/bora-xmlui/handle/1956/7540\n",
    "\n",
    "[2] M. T. Goodrich, R. Tamassia, et M. H. Goldwasser, Data structures and algorithms in Python, 1st edition. Hoboken, N.J: Wiley, 2013.\n",
    "\n",
    "[3] « TimeComplexity - Python Wiki ». Consulted the: 4 janvier 2026. [Online]. Available on: https://wiki.python.org/moin/TimeComplexity\n",
    "\n",
    "[4] R. E. L. Tafurth Garcia, « ChatGPT - COM713 », Transcript. [Online]. Available: https://chatgpt.com/share/695ad5cb-35f8-8008-a3a0-d8b0302b4eb2\n",
    "\n",
    "[5] R. Sedgewick and K. D. Wayne, Algorithms, 4th ed. Upper Saddle River: Addison-Wesley, 2011.\n",
    "\n",
    "[6] S. Cofano, G. Benedetti, and M. Dell’Amico, ‘SBOM Generation Tools in the Python Ecosystem: an In-Detail Analysis’, in 2024 IEEE 23rd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), Sanya, China: IEEE, Dec. 2024, pp. 427–434. doi: 10.1109/TrustCom63139.2024.00077. Dataset: https://github.com/serenacofano/SBOM-python-ecosystem\n",
    "\n",
    "[7] C. Jia, N. Li, K. Yang, and M. Zhou, ‘SIT: An Accurate, Compliant SBOM Generator with Incremental Construction’, in 2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), Ottawa, ON, Canada: IEEE, Apr. 2025, pp. 13–16. doi: 10.1109/ICSE-Companion66252.2025.00013. Dataset: https://zenodo.org/records/13882428\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
